# =============================================================================
# RAG Finance Assistant Configuration
# =============================================================================

# -----------------------------------------------------------------------------
# Embedding Configuration
# -----------------------------------------------------------------------------
EMBEDDING_MODEL=all-MiniLM-L6-v2

# -----------------------------------------------------------------------------
# LLM Configuration
# -----------------------------------------------------------------------------
# Choose your LLM provider: "ollama" (local) or "azure-openai" (cloud)
LLM_PROVIDER=azure-openai

# Azure OpenAI Configuration (only needed if LLM_PROVIDER=azure-openai)
AZURE_OPENAI_BASE_URL=https://ai-proxy.lab.epam.com
AZURE_OPENAI_API_KEY=your_openai_api_key_here
AZURE_OPENAI_API_VERSION=2024-02-01
AZURE_OPENAI_DEPLOYMENT=gpt-4o-mini-2024-07-18

# OpenAI Configuration (only needed if LLM_PROVIDER=openai)
OPENAI_API_KEY=your_openai_api_key_here
OPENAI_MODEL=gpt-4

# Ollama Configuration (only needed if LLM_PROVIDER=ollama)
OLLAMA_MODEL=llama3.2:3b
# Other Ollama options: mistral:7b, phi3:mini, gemma2:2b

# -----------------------------------------------------------------------------
# LangFuse Observability (Self-hosted)
# -----------------------------------------------------------------------------
# Leave empty to disable LangFuse tracing
# After first startup, get keys from http://localhost:3000
LANGFUSE_PUBLIC_KEY=pk-lf-5f15073c10aded8072fe11481b4b6aaf
LANGFUSE_SECRET_KEY=sk-lf-5faea9f9abe8e41438466fe5d28b487d41aa9e2d1272947d62915f43cb21720f

# -----------------------------------------------------------------------------
# Redis Cache Configuration
# -----------------------------------------------------------------------------
REDIS_URL=redis://redis:6379

# Enable LangGraph checkpointing for state persistence (memory between requests)
# Set to "false" to disable checkpointing
CHECKPOINT_ENABLED=true

# Checkpointer type: "memory" (RAM, easier debugging) or "redis" (persistent across restarts)
# memory: Fast, good for development and debugging (state lost on restart)
# redis: Persistent, good for production (state survives restarts)
CHECKPOINTER_TYPE=memory

# Memory service type: "memory" (RAM) or "redis" (persistent across restarts)
# Controls where chat history is stored
# memory: Fast, state lost on restart, good for development
# redis: Persistent, state survives restarts, good for production
MEMORY_TYPE=memory

# Enable node-level caching for performance (skips expensive operations)
# Set to "false" to disable caching
CACHE_ENABLED=true

# -----------------------------------------------------------------------------
# RAG Quality Improvements Configuration
# -----------------------------------------------------------------------------

# Chunking Configuration
# Choose chunking method: "recursive" (fixed-size chunks) or "semantic" (semantic similarity-based)
# - recursive: RecursiveCharacterTextSplitter - fast, predictable chunk sizes
# - semantic: SemanticChunker - context-aware, variable chunk sizes based on meaning
CHUNKING_METHOD=semantic

# RecursiveCharacterTextSplitter settings (only used when CHUNKING_METHOD=recursive)
# Chunk size and overlap for fixed-size chunking
CHUNK_SIZE=500
CHUNK_OVERLAP=50

# SemanticChunker settings (only used when CHUNKING_METHOD=semantic)
# Breakpoint type: "percentile", "standard_deviation", or "interquartile"
# - percentile: Splits when similarity difference exceeds X percentile (e.g., 95th)
# - standard_deviation: Splits when difference exceeds X standard deviations from mean
# - interquartile: Uses interquartile distance to determine split points
SEMANTIC_BREAKPOINT_TYPE=percentile

# Breakpoint threshold value
# - For percentile: 90-99 (higher = fewer, larger chunks)
# - For standard_deviation: 1-3 (higher = fewer, larger chunks)
# - For interquartile: typically 1.5-3.0
SEMANTIC_BREAKPOINT_THRESHOLD=95

# Search Method Configuration
# Choose search method: "vector" (semantic search), "bm25" (keyword search), or "hybrid" (combination)
# - vector: Uses embeddings for semantic similarity (best for conceptual queries)
# - bm25: Uses BM25 keyword matching (best for exact term matching)
# - hybrid: Combines both methods with configurable weighting (best overall)
SEARCH_METHOD=vector

# Hybrid Search Configuration
# Weight for combining vector and BM25 search (only used when SEARCH_METHOD=hybrid)
# 0.0 = pure BM25, 1.0 = pure vector, 0.5 = equal weight
HYBRID_ALPHA=0.5

# Re-ranking Configuration
# Enable re-ranking of retrieved documents using FlashRank
# When enabled, system retrieves more documents (RAG_INITIAL_TOP_K) and re-ranks to select best ones (RAG_FINAL_TOP_K)
# Set to "false" to disable re-ranking for comparison
RERANK_ENABLED=true

# Number of documents to retrieve initially (before re-ranking)
RAG_INITIAL_TOP_K=15

# Number of documents to use after re-ranking
RAG_FINAL_TOP_K=5

# Reranker Configuration
# Choose reranker: "flashrank" (local, fast, free) or "cohere" (API-based, high quality)
# - flashrank: Uses local ms-marco-MiniLM-L-12-v2 model, no API key needed
# - cohere: Uses Cohere's rerank-english-v3.0, requires COHERE_API_KEY
RERANKER_TYPE=cohere

# Cohere API Configuration (only needed if RERANKER_TYPE=cohere)
COHERE_API_KEY=your_cohere_api_key_here
COHERE_RERANK_MODEL=rerank-english-v3.0

